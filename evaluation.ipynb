{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 5.5546715329196825e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/csgrad/sunilruf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example machine-generated and reference sentences\n",
    "machine_generated = \"The cat is on the mat\"\n",
    "reference = \"There is a cat on the mat\"\n",
    "\n",
    "# Tokenize sentences\n",
    "machine_tokens = nltk.word_tokenize(machine_generated.lower())\n",
    "reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu([reference_tokens], machine_tokens)\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (2.1.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (2.1.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (4.34.0)\n",
      "Requirement already satisfied: numpy in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (1.26.1)\n",
      "Requirement already satisfied: requests in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from bert-score) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.2.140)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.17.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from requests->bert-score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from requests->bert-score) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 482/482 [00:00<00:00, 910kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 17.3MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 19.6MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 54.5MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.42G/1.42G [00:18<00:00, 76.3MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 108.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.32 seconds, 0.76 sentences/sec\n",
      "Precision: 0.9197907447814941\n",
      "Recall: 0.9149471521377563\n",
      "F1 Score: 0.9173625707626343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Example machine-generated and reference sentences\n",
    "machine_generated = [\"The cat is on the mat\"]\n",
    "reference = [\"There is a cat on the mat\"]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(cands=machine_generated, refs=reference, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"Precision:\", P.mean().item())\n",
    "print(\"Recall:\", R.mean().item())\n",
    "print(\"F1 Score:\", F1.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 310.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.05 seconds, 21.88 sentences/sec\n",
      "Precision: 0.8327552676200867\n",
      "Recall: 0.9278967380523682\n",
      "F1 Score: 0.8777554035186768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example machine-generated and reference sentences\n",
    "machine_generated = [\"This policy explains the universityâ€™s operational practices with respect to visitor information collected from official University at Buffalo websites and associated third-party web applications. The following is the Policy Statement: The University at Buffalo (UB, university) is committed to protecting visitorâ€™s privacy when navigating through official University at Buffalo websites and associated third-party web applications. Visitors navigate through a majority of official UB websites and associated third-party web applications without providing personal information. However, the university implements operational practices to enhance the ease and efficiency with which visitors interact with official UB websites and associated third-party web applications. To that end: UB collects information for purposes including, but not limited to providing requested services and analyzing web traffic UB collects voluntarily-provided information, which may include personal information; this information is collected through processes including, but not limited to sending an email; filling out a webform, survey, or application; or completing a financial transaction. This policy is consistent with federal and state laws, rules and regulations, policies and procedures of the State University of New York (SUNY). This policy is consistent with the provisions of the Internet Security and Privacy Act, the New York State Freedom of Information Law (FOIL), Family Educational Rights and Privacy Act (FERPA), and the Personal Privacy Protection Law.\"]\n",
    "reference = [\" The University at Buffalo's Website privacy policy states that the university is committed to protecting visitors' privacy when navigating through official University at Buffalo websites and associated third-party web applications. The policy explains how the university collects, uses, and protects visitor information.\"]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(cands=machine_generated, refs=reference, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"Precision:\", P.mean().item())\n",
    "print(\"Recall:\", R.mean().item())\n",
    "print(\"F1 Score:\", F1.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 482/482 [00:00<00:00, 1.27MB/s]\n",
      "Downloading vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 15.7MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 38.2MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 82.1MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.42G/1.42G [00:24<00:00, 58.6MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.24 seconds, 0.45 sentences/sec\n",
      "Precision: 0.9393523335456848\n",
      "Recall: 0.9175694584846497\n",
      "F1 Score: 0.9283331036567688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example machine-generated and reference sentences\n",
    "from bert_score import score\n",
    "\n",
    "machine_generated = [\"The President of University at Buffalo is Satish K Tripathi\"]\n",
    "reference = [\"The President of University at Buffalo is Venu Govindaraju\"]\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(cands=machine_generated, refs=reference, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"Precision:\", P.mean().item())\n",
    "print(\"Recall:\", R.mean().item())\n",
    "print(\"F1 Score:\", F1.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1020: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:08<00:00, 42.95s/it]\n",
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "#Deci/DeciLM-6b-instruct\n",
    "#meta-llama/Llama-2-13b-chat-hf\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = 'YOUR_HF_TOKEN'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    #quantization_config=bnb_config,\n",
    "    #device=\"cuda:0\",\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.2,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_score_string\",\n",
    "    criteria=accuracy_criteria,\n",
    "     llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"Explanation: The assistant's response is completely unrelated to the question asked. The user asked about the university's website privacy policy, but the assistant provided information about the president of the university. There is no mention or relevance to the website privacy policy in the response.\\n\\nRating: [[1]]\", 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "# Correct\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    #prediction=\"This policy explains the universityâ€™s operational practices with respect to visitor information collected from official University at Buffalo websites and associated third-party web applications. The following is the Policy Statement: The University at Buffalo is committed to protecting visitors privacy when navigating through official University at Buffalo websites and associated third-party web applications. Visitors navigate through a majority of official UB websites and associated third-party web applications without providing personal information. However, the university implements operational practices to enhance the ease and efficiency with which visitors interact with official UB websites and associated third-party web applications. To that end: UB collects information for purposes including, but not limited to providing requested services and analyzing web traffic UB collects voluntarily-provided information, which may include personal information; this information is collected through processes including, but not limited to sending an email; filling out a webform, survey, or application; or completing a financial transaction. This policy is consistent with federal and state laws, rules and regulations, policies and procedures of the State University of New York. This policy is consistent with the provisions of the Internet Security and Privacy Act, the New York State Freedom of Information Law, Family Educational Rights and Privacy Act (FERPA), and the Personal Privacy Protection Law.\",\n",
    "    #reference=\"The University at Buffalo's Website privacy policy states that the university is committed to protecting visitors' privacy when navigating through official University at Buffalo websites and associated third-party web applications. The policy explains how the university collects, uses, and protects visitor information.\",\n",
    "    prediction = \"The president of University at Buffalo is Satish K Tripathi\",\n",
    "    reference = \"The president of University at Buffalo is Venu Govindaraju\",\n",
    "    input=\"What is the University at Buffalo's Website privacy policy?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "hf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.17828356144386548}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction=\"This policy explains the universityâ€™s operational practices with respect to visitor information collected from official University at Buffalo websites and associated third-party web applications. The following is the Policy Statement: The University at Buffalo is committed to protecting visitors privacy when navigating through official University at Buffalo websites and associated third-party web applications. Visitors navigate through a majority of official UB websites and associated third-party web applications without providing personal information. However, the university implements operational practices to enhance the ease and efficiency with which visitors interact with official UB websites and associated third-party web applications. To that end: UB collects information for purposes including, but not limited to providing requested services and analyzing web traffic UB collects voluntarily-provided information, which may include personal information; this information is collected through processes including, but not limited to sending an email; filling out a webform, survey, or application; or completing a financial transaction. This policy is consistent with federal and state laws, rules and regulations, policies and procedures of the State University of New York. This policy is consistent with the provisions of the Internet Security and Privacy Act, the New York State Freedom of Information Law, Family Educational Rights and Privacy Act (FERPA), and the Personal Privacy Protection Law.\",\n",
    "#reference=\"The University at Buffalo's Website privacy policy states that the university is committed to protecting visitors' privacy when navigating through official University at Buffalo websites and associated third-party web applications. The policy explains how the university collects, uses, and protects visitor information.\",\n",
    "#prediction = \"Address: 12 Capen Hall, Buffalo, New York 14260-1660. Phone: 716-645-2000\"\n",
    "#reference = \"Address: 12 Capen Hall, Buffalo, New York 14260-1660 Phone: 716-645-2266 Fax: 716-645-3952 Email: diversity@buffalo.edu\"   \n",
    "prediction = \"The president of University at Buffalo is Satish K Tripathi\"\n",
    "reference = \"The president of University at Buffalo is Venu Govindaraju\"\n",
    "hf_evaluator.evaluate_strings(prediction=prediction, reference=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "        Given below is the criteria to evalaute the answer generated with its reference:\n",
    "        Score 1: The answer is completely unrelated to the reference.\n",
    "        Score 3: The answer has minor relevance but does not align with the reference.\n",
    "        Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "        Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "        Score 10: The answer is completely accurate and aligns perfectly with the reference.\n",
    "        \n",
    "        Prediction:{prediction}\n",
    "        Reference:{reference}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def evaluate(self, prediction, reference):\\n        score = 0\\n        if prediction == reference:\\n            score = 10\\n        elif prediction is None or reference is None:\\n            score = 0\\n        else:\\n            score = self.evaluate_similarity(prediction, reference)\\n        return score\\n\\n    def evaluate_similarity(self, prediction, reference):\\n        similarity = 0\\n        if len(prediction) > 0 and len(reference) > 0:\\n            similarity = self.levenshtein_distance(prediction, reference)\\n        return similarity\\n\\n    def levenshtein_distance(self, prediction, reference):\\n        d = levenshtein_distance(prediction, reference)\\n        return d\\n```\\n\\nHere\\'s an example usage of the `Evaluator` class:\\n```\\nevaluator = Evaluator()\\n\\n# Example input and reference\\ninput_text = \"The quick brown fox jumps over the lazy dog.\"\\nreference_text = \"The quick brown fox jumped over the lazy dog.\"\\n\\n# Generate a prediction\\npredictions = [\"The quick brown fox jumped over the lazy cat.\", \"The quick brown fox ran over the lazy dog.\", \"The quick brown fox hopped over the lazy dog.\"]\\n\\n# Evaluate the predictions\\nfor prediction in predictions:\\n    print(f\"Prediction: {prediction}\")\\n    print(f\"Reference: {reference_text}\")\\n    score = evaluator.evaluate(prediction, reference_text)\\n    print(f\"Score: {score}/10\")\\n```\\nThis code will generate three predictions and evaluate their similarity to the reference text using the Levenshtein distance metric. The output will show the prediction, the reference text, and the score out of 10 based on how similar the prediction is to the reference.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
